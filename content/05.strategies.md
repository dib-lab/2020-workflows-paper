## Strategies to get the most out of your workflows

### Developing steps that go into your workflow

#### Build your workflow using subsampled data

It is rare to find a workflow that will analyze your data from start to finish without testing, troubleshooting, and iteration.
Testing each step with a small dataset prior to running the full analysis greatly facilitates workflow design and saves resources.
After installing a program, if the program comes with test data, run it and check results against the expected results, to verify that it is working on your system.
After that, subsample your own data and check you can run the program on this subsampled data.
For example, if working with FASTQ data, you can subsample the first million lines of your data (first 250k reads) by running:

"`
head -n 1000000 FASTQ_FILE.fq > test_fastq.fq
"`

While there are many more sophisticated ways to subsample reads, this technique should be sufficient for testing each step of a workflow prior to running your full dataset.
Note, some programs will fail with too few reads or too few results, so be sure to examine that possibility if running into errors at this stage, either in the literature, program manual, or by running larger subsets of data.
% add sentence here to reiterate concept of testing on public data!...and that workflows make this easy.

### Computational Notebooks 

Computational notebooks allow users to combine narrative, code, and code output (e.g. visualizations) in a single location, enabling the user to conduct analysis and visually assess the results in a single file.
Jupyter notebooks and Rmarkdown are the two most popular notebook platforms (see **Figure @fig:nb_figure**) [@doi:10.3233/978-1-61499-649-1-87; @url:https://rmarkdown.rstudio.com/].
Notebooks are particularly useful for data exploration and developing visualizations prior to integration into a workflow or as a report generated by a workflow that can be shared with collaborators. 


![**Examples of computational notebooks.** Computational notebooks allow the user to mix text, code, and results in one document. **A** shows a raw RMarkdown document viewed in the RStudio integrated development environment, while **B** shows a rendered HTML file produced by knitting the RMarkdown. **C** shows a Jupyter Notebook, where code, text, and results are rendered inline as each code chunk is executed. The second grey chunk is a raw markdown chunk with text that will be rendered inline when executed. Both notebooks generate a histogram of a metadata feature, number of generations, from a long-term evolution experiment with *Escherichia coli* [@doi:10.1038/nature18959]. Computational notebooks facilitate sharing by packaging narrative, code, and visualizations together.](images/nb_figure.png){#fig:nb_figure}

### Version Control

As your project develops, version control allows you to keep track of changes over time.
You may already do this in some ways, perhaps with frequent hard drive backups or by manually saving different versions of the same file  - e.g. by appending the date to a script name or appending "version_1" or "version_FINAL" to a manuscript draft.
For computational workflows that will inevitably undergo multiple changes in parameters, data, visualizations, and analysis, it is essential to keep track of which analysis produced which output files, so that the final analysis is reproducible.
However, version control can also save time and effort at every step.
If a key piece of a workflow inexplicably stops working, good version control can allow you to rewind in time and identify differences from when the pipeline worked to when it stopped working.
If multiple people are working on the same project, version control can both avoid conflict and ensure that no productivity is lost.

Version control systems such as Git or Mercurial can be used to properly keep track of all changes over time, even across multiple users, scripting languages, and including visualizations.
In particular, Git has emerged as the dominant version control system for biological code, particularly when combined with online repositories such as Github, GitLab, or Bitbucket, which store online version histories for all tracked files [@doi:10.1186/1751-0473-8-7; @doi:10.1371/journal.pcbi.1004668].
In addition to acting as an additional backup location, the online services support drag-and-drop file addition and full control over the repository using the web interface, which greatly lowers the barrier to getting started with version control systems.
While these systems do not work well with Google Docs or Microsoft Word, they can greatly simplify asynchronous collaborative manuscript writing when combined with services such as Overleaf and Manubot [@doi:10.1371/journal.pcbi.1007128].

Git version control is primarily designed to handle small text files, but version control also exists for data sets.
Data version control can be used to store a read-only copy of raw sequencing files and accompanying metadata, or to keep track of difference in intermediate files that change with tool paramters or versions.
The Open Science Framework (OSF) [@doi:10.5195/JMLA.2017.88], maintained and developed by the Center for Open Science, provides free storage of an unlimited number of files up to 5GB each in size, and allows the user to keep the data private until they are ready to share (make the project public).
OSF provides built-in version control and is supported by a data preservation fund that will keep the data available for 50+ years.
While OSF and other similar repositories (e.g. figshare) are suitable for use at any stage of a research project, repositories such as Zenodo and the Dryad Digital Repository (Dryad), are designed to make publication-ready data discoverable, citable, and reusable.
Other services are compatible with git version control, e.g. Git Large File Storage (LFS) and Data Version Control (DVC).

These version control systems can also facilitate code and data availability and reproducibility for publication.
For example, to ensure the correct version of the code is preserved, you can create a "release", a snapshot of the current code and files in a GitHub repository.
You can then generate a DOI for that release using Zenodo and make it available to reviewers and beyond (see "sharing" section, below).

![**Version Control** In this example, a typo in version 1 (in red) was corrected (green). Version control systems such as git and mercurial track line-by-line changes and store the information. These systems are particularly useful to handle accidental deletions or early code or text you'd like to return to after deletion.](images/version_control.pdf){#fig:version_control}

### Documenting reproducible workflows for yourself and others

As with experimental biology, it is essential to write down everything you do - that is, record the origin of every file (e.g. download URL) and all metadata, record the version of software and each parameter you used, record any manual filtering or data preprocessing steps, and keep track of the order in which you executed each program.
Without the ability to fully examine and reproduce your analysis, it will be impossible for you or your collaborators to assess whether the results are accurate, or even to understand how the heuristic decisions you took impact the conclusions made from an analysis.

Computational project management is a learned skill that will take time to implement. There are a myriad of ways to document your computational work, and you'll need to experiment with the ways that work for you.
For some portions of your project, you may want to document your work using a narrative approach, using written language to detail what steps you took and to communicate how the steps relate to one another.
For other portions, it may be more useful to keep short, bulleted notes with your code or intersperse your commands with helpful diagrams.
What is most important is to develop a clear documentation strategy and stick with it tenaciously.
While the preferred tools discussed below will certainly change over time, these principles apply broadly and will help you design clear, well-documented, and reproducible analyses.

#### Use consistent and descriptive names

Consistent, descriptive names keep your project organized and interpretable for yourself and collaborators.
This applies to your files, your scripts, your variables, your workflows, your manuscripts, and even your projects, each of which should have a unique and descriptive identifier.
Since the number of files in data-intensive biology can quickly get out of hand, consistent file naming is especially important.
For example, you can implement a numbering scheme for your files, where the first file in your analysis starts with "00", the next with "01", etc.
You can also append the tool name to output to make it clear where the file came from.
Additionally, using a standardized yet flexible folder structure from the outset of your project facilitates file organization, even as a project becomes increasingly complex.
Keeping independent portions of your analysis in descriptive folders can help keep your project workspace clean and organized.
Within your files, using consistent and descriptive variable names will help build a readable codebase.

![filenaming caption goes here](images/filenaming.pdf){#fig:filenaming}

#### Store metadata about your workflow with your workflow

Biological analyses often span hundreds of steps and involve many small decisions: What parameters for each step?
Why did you use a certain reference file for annotation as compared with other available files?
How did you finally manage to get around the program or installation error?
All of these pieces of information contextualize your results and may be helpful when writing your manuscript.
Keeping information about these decisions in an intuitive and easily accessible place helps you find it when you need it.
Each main directory should include notes on the data or scripts contained within, so that a collaborator could look into the directory and understand what to find there (especially since that collaborator is likely to be you, a few months from now!).
Code itself can be (or contain) documentation - you can add comments with the reasoning behind parameter choice or include a link to the seqanswers post that helped you decide how to shape your differential expression analysis.
Larger pieces of information can be kept in "README" or notes documents kept alongside your code and other documents.
For example, a GitHub repository documenting the reanalysis of the Marine Microbial Eukaryote Transcriptome Sequencing Project uses a README alongside the code to document the workflow and digital object identifiers for data products [@url:https://github.com/dib-lab/dib-MMETSP; @doi:10.1093/gigascience/giy158].

#### Add visual representations
Visual representations illustrate the connections in a workflow.
At the highest level, flowcharts that detail relationships between steps of a workflow can help provide big-picture clarification, especially when the pipeline is complicated.
For individual steps, a graphical representation of the output can show the status of the project or provide insight on additional analyses that should be added.
Whenever possible, adding visualizations can help improve the readability and reproducibility of your project. Figure {@fig:sgc_workflow} illustrates a workflow visualization modified from a graph produced by the workflow software Snakemake [@doi:10.1101/462788].

![A directed acyclic graph (DAG) that illustrates connections between all steps of a sequencing data analysis workflow. Each box represents a step in the workflow, while lines connect sequential steps. 
The DAG shown in this figure illustrates a real bioinformatics workflow and was generated by modifying the default Snakemake workflow DAG [@doi:10.1101/462788]. 
The colors represent arms of the workflow that achieve a final result, such as a multiple sequence alignment of a protein of interest. 
While the workflow is complex, it is coordinated by a workflow system, alleviating the need for a user to manage file interdependencies.](images/hu_dag.png){#fig:sgc_workflow}


### Sharing Your Reproducible Analyses

Sharing your workflow is a useful way to communicate every step you took in a data analysis pipeline.
Your collaborators, peer reviewers, and scientists seeking to use a similar method as your own will all benefit from open and accessible code.
Sticking to a clear documentation strategy, using a version control system, and packaging your code in notebooks or as a workflow prepare them to be easily shared with others.
However, sharing code in this way can still be burdensome for others to interact with given the need for software installation and differences in user operating systems.
Tools like Binder, Whole Tale, and Shiny apps can reduce the time to reproduction by other scientists by constructing controlled environments identical to those in which the original computation was performed [@doi:10.25080/Majora-4af1f417-011; @doi:10.1016/j.future.2017.12.029].
These tools substantially reduce overhead associated with interacting with someones code base and data, and in doing so, make it fast and easy to rerun portions of the analysis, check accuracy, or even tweak the analysis to produce new results.
These tools are also great for teaching, as they provide consistent learner interfaces and environments.

#### sharing greybox

 **Binder (mybinder.org)** Binder is a tool that makes a GitHub repository executable in a specified environment [@doi:10.25080/Majora-4af1f417-011]. 
It uses package management by R, pip, or conda to build a docker container with the software required to run the analyses contained in a GitHub repository. 
This is especially useful for computational notebooks. 
The binder can then be shared with collaborators (or students in a classroom setting), where the analysis or visualizations can be reproduced using your provided code. 
Binder instances are not static and can be modified by collaborators during a binder session. 
The underlying code will not be changed unless changes are committed into the original GitHub repository, preserving the reproducibility of the original analysis.
 
**Shiny Apps** Shiny is a tool that allows you to build interactive web pages using R code. 
It allows you to package data that is manipulated  by R code in real-time in a web page, producing analysis and visualizations of a data set. 
Shiny apps can contain user-specifiable parameters, allowing a user to control visualizations or analyses. 
For example, if a Shiny app contained RNA-seq differential expression data, it might allow the user to specify which gene counts it should plot. 
Shiny apps allow collaborators who may or may not know R to change R visualisations to fit their interests.   
 
**Other tools** There are many other tools that aid in reproducibility or sharing of results. 
Some tools, such as Whole Tale, aim to package the entire research object by capturing data, code and software environments in one executable package [@doi:10.1016/j.future.2017.12.029]. 
Other tools such as Vega-lite and plotly produce single interactive visualizations that can be shared with collaborators or integrated into websites [@doi:10.1109/TVCG.2016.2599030; @url:https://plotly.com/].


### Scaling Workflows

#### Assess required resources

Bioinformatic tools vary in the resources they require: some analysis steps are compute-intensive, other steps are memory intensive, and still others will have large intermediate storage needs.
While it can be difficult to estimate resources required for each tool, workflow systems provide built-in tools to monitor resource usage for each step.
This reporting can be used while running a workflow on a single or a few samples to estimate required resources.
These resources can then be specified to run the workflow on all samples.

#### Scale workflows with tools that leverage computational approximations

Many bioinformatics workflows take a long time and significant computational resources to run, and interpretable results are often only produced by the last few steps.
This means that time-to-insight from sequencing data is often very high.

Understanding the basic structure of data, the relationship between samples, and the approximate composition of each sample is very helpful at the beginning of data analysis, and can often drive analysis decisions in different directions than those originally intended.
Although most bioinformatics workflows generate these types of insights, there are a few tools that do so rapidly, allowing the user to generate quick hypotheses that can be further tested by more extensive, fine-grained analyses.

**Sketching** Sketching algorithms work with compressed approximate representations of sequencing data and thereby reduce runtimes and computational resources.
These approximate representations retain enough information about the original sequence to recapitulate the main findings from many exact but computationally intensive workflows.
Most sketching algorithms estimate sequence similarity in some way, allowing the user to gain insights from these comparisons.
For example, sketching algorithms can be used to estimate all-by-all sample similarity which can be visualized as a Principle Component Analysis or a multidimensional scaling plot, or can be used to build a phylogenetic tree with accurate topology.
Sketching algorithms also dramatically reduce the runtime for comparisons against databases (e.g. all of GenBank), allowing users to quickly compare their data against large public databases.
Sketching algorithms have been reviewed in-depth by Rowe [@doi:10.1186/s40168-019-0653-2].

**Read quasi-mapping vs alignment** RNA-seq analysis approaches like differential expression or transcript clustering rely on transcript or gene counts.
Many tools can be used to generate these counts by quantifying the number of reads that overlap with each transcript or gene.
For example, tools like STAR and HISAT2 produce alignments that can be post-processed to generate per-transcript read counts [@doi:10.1093/bioinformatics/bts635; @doi:10.1038/s41587-019-0201-4].
However, these tools generate information-rich output, specifying per-base alignments for each read.
Quasi-mapping produces the minimum information necessary for read quantification, thereby reducing the time and resources needed to generate and store read count information [@doi:10.1093/bioinformatics/btw277].

discuss blast approximations?

