## Data and resource management for workflow-enabled biology

Massive increases in sequencing capacity have increased the sequencing data available for biological query [@url:https://www.ncbi.nlm.nih.gov/sra/docs/sragrowth/].
Fortunately, by automating many of the time-intensive project management steps traditionally required for data-intensive biology, workflow systems increase our capacity for data analysis.
As with experimental biology, more data is not always better.
Dedicating time to carefully design your sequencing analysis and select or sequence appropriate data will save valuable time and resources during analysis.
Below, we provide recommendations for sequence analysis data acquisition, management, and quality control that have become especially important as the scale of data has increased.
We include practical strategies for computational resource management and briefly discuss tools for critically assessing your data and quickly gaining insights from large datasets.

### Managing large-scale datasets

large scale data is hard.

#### Carefully select your data

As with all biological analyses, a critical step of sequence analysis is obtaining high-quality data for your scientific question.
With vast amounts of sequencing data already available in public repositories, it is often possible to begin investigating your research question by seeking out publicly available data.
In some cases, these data will be sufficient to conduct your entire analysis.
In others cases, particularly for biologists conducting novel experiments, these data can inform decisions about sequencing type, depth, and replication, and can help uncover potential pitfalls before they cost valuable time and resources.

##### Accessing publicly-available data

Most journals now require data for all manuscripts to be made accessible, either at publication or after a short moratorium.
You can find relevant sequencing data either by starting from the "data accessibility" sections of papers relevant to your research or by directly searching for your organism, environment, or treatment of choice in public data portals and repositories.

The International Nucleotide Sequence Database Collaboration (INSDC), which includes the Sequence Read Archive (SRA), European Nucleotide Archive (ENA), and DataBank of Japan (DDBJ) is the largest repository for raw sequencing data [@doi:10.1093/nar/gkv1323].
Additional curated databases focus on processed data instead, such as gene expression in the Gene Expression Omnibus (GEO) [@doi:10.1093/nar/30.1.207].
Organism-specific databases such as **Wormbase** (*Caenorhabditis elegas*) specialize on curating and integrating sequencing and other data associated with a model organism [@doi:10.1093/nar/gkz920].
The SRA, ENA, and DDBJ no longer accept raw sequencing data from large consortia projects, so data from these efforts are often hosted in consortia-specific databases such as those hosted by the Tara Ocean Foundation [@doi:10.1038/sdata.2015.23].
Unlike the SRA and associated databases which are centralized and searchable, databases overseen by consortia often require domain-specific knowledge and have unique download and authentication protocols.
Finally, rather than focusing on certain data types or organisms, some repositories are designed to hold any data and metadata associated with a specific project or manuscript (e.g. Open Science Framework, Dryad, Zenodo [@doi:10.5195/JMLA.2017.88]).

##### Generating your own data

If generating your own data, proper experimental design and planning are essential.
For cost-intensive sequencing data, there are a range of decisions about experimental design and sequencing (including sequencing type, sequencing depth per sample, and biological replication) that impact your ability to properly address your research question.
These considerations will be different for different types of sequence analysis.
While we have curated a series of domain-specific references that may be useful as go about designing your experiment (see **Table @tbl:seq_resources**), conducting discussions with experienced bioinformaticians and statisticians, **prior to beginning your experiments** if possible, is the best way to ensure you will have sufficient statistical power to detect effects.
Given the resources invested in collecting samples for sequencing, it's important to build in a buffer to preserve your experimental design in the face of unexpected laboratory or technical issues.


|  Sequencing type | Resources |
| --- | --- |
|  RNA-sequencing | [@doi:10.1186/s13059-016-0881-8; @doi:10.1261/rna.058339.116; @doi:10.1261/rna.046011.114] |
|  Metagenomic sequencing | [@doi:10.1038/nbt.2235; @doi:10.1038/nbt.3935; @doi:10.1016/j.tim.2018.11.003]|
|  Amplicon sequencing | [@doi:10.7554/eLife.46923; @doi:10.1371/journal.pone.0124671; @doi:10.1038/nbt.3981] |
|  Microbial isolate sequencing | [@doi:10.1038/srep08747] |
|  Eurkaryotic genome sequencing |  |
|  Whole-genome resequencing | [@doi:10.1111/mec.14264] |
|  Rad seq |  |
|  Chip seq |  |
|  ATAC seq |  |
|  single cell RNA-seq | [@doi:10.1186/s13059-016-0927-y; @doi:10.1186/s13073-017-0467-4] |
|  ? |  |
Table: References for experimental design and considerations for common sequencing chemistries. {#tbl:seq_resources}


As your experiment progresses, keep track of as much information as possible -- dates and times of sample collection, storage, and extraction, sample names, aberrations that occurred during collection, kit lot used for extraction, and any other sample measurements you might be able to obtain (temperature, location, metabolite concentration, name of collector, etc).
This metadata allows you to keep track of your samples, to control for batch effects that may arise from unintended batching during sampling or experimental procedures and makes the data you collect reusable for future applications and analysis by yourself and others.
When working with metadata, using a standardized format that is easy for a computer to read can simplify downstream analysis.
Standard guidelines for formatting data for scientific computing are given in [@doi:10.1371/journal.pcbi.1005510].

#### Protect your valuable data

  - back up raw data in multiple locations!!
  - write Protect
  - don't need version control, but may want to store files on OSF
  - you may want to keep intermediate files that take a long time to generate

Workflow systems allow users to generate an automatable, repeatable workflow.
This removes the necessity of storing some intermediate files as these can be easily regenerated.
However, storing or backing up raw data or computationally intensive results can be a burden in data intensive workflows.
Many universities provide cloud storage space (e.g. through google drive, box, dropbox etc), and researchers can pay for individual storage on these services, or services attached to cloud computing (e.g. Amazon Web Services).
Full computer backups can be conducted to these storage locations (e.g. with rclone [@doi:10.1111/2041-210X.12550]), or there are also a number of paid services that will conduct backups at regular intervals (e.g. Backblaze, Dropbox Pro).
Free online repositories mentioned in the version control section like OSF, Dryad, and Zenodo can also store or backup important files.

Regardless of which storage or backup strategy you choose, it is a good idea to have multiple independent backups of raw data and workflows; these cannot be easily regenerated if lost to a computer failure or other unforseeable event like a lab fire.

% Link out to data/project management info? e.g. Many of these considerations are addressed in a data carpentry lesson (https://datacarpentry.org/organization-genomics/).

**moved from 05.strategies**
This type of version control is primarily designed to handle small text files, but version control also exists for larger files and datasets.

You should not need version control for raw datasets, which

Data version control can be used to store a read-only copy of raw sequencing files and accompanying metadata, or to keep track of difference in intermediate files that change with tool parameters or versions.
The Open Science Framework (OSF) [@doi:10.5195/JMLA.2017.88], maintained and developed by the Center for Open Science, provides free storage of an unlimited number of files up to 5GB each in size, and allows the user to keep the data private until they are ready to share (make the project public).
OSF provides built-in version control and is supported by a data preservation fund that will keep the data available for 50+ years.
Other services are compatible with git version control, e.g. Git Large File Storage (LFS) and Data Version Control (DVC).

While OSF and other similar repositories (e.g. figshare) are suitable for use at any stage of a research project, repositories such as Zenodo and the Dryad Digital Repository (Dryad), are designed to make publication-ready data discoverable, citable, and reusable.


#### Ensure data integrity during transfers

If you're working with publicly-available data, you may be able to work on a compute system where the data are already available, circumventing time and effort required for downloading and moving the data.
Databases such as the Sequence Read Archive (SRA) are now available on commercial cloud computing systems, and open source projects such as Galaxy enable import and work on SRA sequence files directly from a web browser [@doi:10.1093/nar/gky379; @url:https://www.ncbi.nlm.nih.gov/sra/docs/sra-cloud/].
Ongoing projects such as the NIH Common Fund Data Ecosystem, aim to develop a data portal to make NIH Common Fund data, including biomedical sequencing data, more findable, accessible, interoperable, and reusable (FAIR).

In most cases, you'll still need to transfer some data - either downloading raw data or transferring important intermediate and results files for backup and sharing (or both).
Checksums can be used to to check file integrity and ensure proper transfer (see **Figure @fig:checksum**).
File compression (gzip, bzip2, BAM/CRAM, etc.) can improve transfer speed and save space.
Tools like Rsync and Rclone automate file transfer between computers or between computers and remote storage providers [@doi:10.1111/2041-210X.12550].
These tools automatically use checksums to verify that files were transferred properly.
Some GUI file transfer tools (e.g. cyberduck) also assess checksums when they are provided.


![**Use Checksums to ensure file integrity** Checksum programs (e.g. md5, sha256) encode file size in a single value known as a "checksum". For any file, this value will be identical across platforms when calculated using the same checksum program. When transferring files, calculate the value of the checksum prior to transfer, and then again after transfer. If the value is not identical, there was an error introduced during transfer (e.g. file truncation, etc).
For publicly-available files, a checksum value is often provided, so that you can check the integrity of the file after download.](images/checksum.svg){#fig:checksum height=3in}

### Securing and managing appropriate computational resources

Sequence analysis requires access to computing systems with adequate storage and analysis power for your data.
For some smaller-scale datasets, local desktop or even laptop systems can be sufficient, especially if using tools that implement data-reduction strategies such as minhashing [@doi:10.1186/s40168-019-0653-2].
However, larger projects require additional computing power, or may be restricted to certain operating systems (e.g. linux).
For these projects, solutions range from research-focused high performance computing systems to research-integrated commercial analysis platforms.
Both research-only and  and commercial clusters provide avenues for research and educational proposals to enable access to their computing resources (see **Table @tbl:computational_resources**).
In preparing for data analysis, be sure to allocate sufficient computational resources and funding for storage and analysis, including large intermediate files and resources required for personnel training.

|  Cloud Provider | Standard Model | Limits |
| --- | --- | --- |
|  Amazon Web Services | Paid |  |
|  Bionimbus Protected Data Cloud | Research allocation | users with eRA commons account |
|  Cyverse Atmosphere | Free with limits | storage and compute hours |
|  EGI federated cloud | Access by contact | European partner countries |
|  Galaxy | Free with storage limits | data storage limits |
|  Google Cloud Platform | Paid |  |
|  Google Colab | Free | computational notebooks, no resource guaruntees |
|  Microsoft Azure | Paid |  |
|  NSF XSEDE | Research allocation | USA researchers or collaborators |
|  Open Science Data Cloud | Research allocation |  |
|  Wasabi | Paid | data storage solution only |
Table: **Research cloud resources** Cloud provider indicates the name of the cloud, standard model indicates the most common route toward using the cloud, and limitations indicates limitations in access or services provided by the cloud. {#tbl:computational_resources}

**add something about workflow-based resource management and/or hpc-integration**

### Getting started: practical considerations for sequencing data

The adage, "garbage in, garbage out" describes most sequencing data analysis: the quality of the input data determines the quality of the output results.
This is true whether your workflow analyzes six samples or 600 samples.
Assessing data at every analysis step can reveal problems and errors early.
You are the single most effective quality control tool that you have, so its important to interrogate your data to search for problems.
While simply looking at your data is sometimes sufficient to catch issues, visualization and software tools make quality control easier.

#### Critically assess your data / Properly quality control your data

Quality control can be as simple as looking at the first few and last few lines of input and output data files, or checking the size of those files (see **Table @tbl:bash_commands**).
To develop an intuition for what proper inputs and outputs look like for a given tool, it is often helpful to first run the test example or data that is packaged with the software.
Comparing these input and output file formats to your own data can help identify and address inconsistencies.

Visualization is another powerful way to pick out unusual or unexpected patterns.
Although large abnormalities may be clear from looking at files, others may be small and difficult to find.
Visualizing raw sequencing data with FastQC (**Figure {@fig:multiqc}B**) and processed sequencing data with tools like the Integrative Genome Viewer and plotting tabular results files using python or R can make aberrant or inconsistent results easier to track down [@url:https://www.bioinformatics.babraham.ac.uk/projects/fastqc/; @doi:10.1093/bib/bbs017].

Many tools generate log files or messages while running.
These files contain information about the quantity, quality, and results from the run, or error messages about why a run failed.
Inspecting these files can be helpful to make sure tools ran properly and consistently, or to debug failed runs.
Parsing and visualizing log files with a tool like MultiQC can improve interpretability of program-specific log files (**Figure @fig:multiqc** [@doi:10.1093/bioinformatics/btw354]).


|  command | function | example |
| --- | --- | --- |
|  ls -lh | list files with information in a human-readable format | ls -lh \*fastq.gz |
|  head | print the first 6 lines of a file to standard out | head samples.csv |
|  tail | print the last 6 lines of a file to standard out | tail samples.csv |
|  less | show the contents of a file in a scrollable screen | less samples.csv |
|  zless | show the contents of a gzipped file in a scrollable screen | zless sample1.fastq.gz |
|  wc -l | count the number of lines in a file | wc -l ecoli.fasta |
|  cat | print a file to standard out | cat samples.csv |
|  grep | find matching text and print the line to standard out | grep ">" ecoli.fasta |
|  cut | cut columns from a table | cut -d"," -f1 samples.csv |
Table: Some bash commands are useful to quickly explore the contents of a file. By using these commands, the user can detect common formatting problems or other abnormalities. {#tbl:bash_commands}


![**Visualizations produced by MultiQC.**
**A** MultiQC summary of FastQC Per Sequence GC Content for 1905 metagenome samples. FastQC provides quality control measurements and visualizations for raw sequencing data, and is a near-universal first step in sequencing data analysis because of the insights it provides  [@url:https://www.bioinformatics.babraham.ac.uk/projects/fastqc/; @doi:10.1093/bib/bbs017].
FastQC measures and summarizes 10 quality metrics and provides recommendations for whether the sample is within an acceptable quality range.
Not all metrics readily apply to all sequencing data types. For example, while multiple GC peaks might be concerning in whole genome sequencing of a bacterial isolate, we would expect a non-normal distrubtion for some metagenome samples that contain organisms with diverse GC content.
Samples like this can be seen in red in this figure.
**B** MultiQC summary of Salmon *quant* reads mapped per sample for RNA-seq samples [@doi:10.1038/nmeth.4197]. MultiQC finds and automatically parses log files from other tools and generates a combined report and parsed data tables that include all samples. MultiQC currently supports 88 tools. In this figure, we see that MultiQC summarizes the number of reads mapped and percent of reads mapped, two values that are reported in the Salmon log files.
](images/multiqc.svg){#fig:multiqc}

#### Handling common biases in sequencing data

Biases in sequencing data originate from experimental design, methodology, sequencing chemistry, or workflows, and are helpful to target specifically with quality control measures.
For example, PCR duplicates can cause problems in libraries that underwent an amplification step, and often need to be removed prior to downstream analysis [@doi:10.1038/nrg3788; @doi:10.1038/srep25533; @doi:10.1086/BBLv227n2p146; @doi:10.1186/s12864-018-4933-1; @doi:10.1186/s13059-014-0420-4].
Contamination can arise during sample collection, nucleotide extraction, library prepartion, or through sequencing spike-ins like PhiX, and could change data interpretation if not removed [@doi:10.1073/pnas.1510461112; @doi:10.1073/pnas.1600338113; @doi:10.1186/1944-3277-10-18].
Libraries sequenced with high concentrations of free adapters or with low concentration samples may have increased barcode hopping, leading to contamination between samples [@doi:10.1111/1755-0998.13009].
Stringent trimming of RNA-sequencing data may reduce isoform discovery [@doi:10.3389/fgene.2014.00013].
The exact biases in a specific data set or workflow will vary greatly between experiments.
To determine what issues are most likely to plague your specific data set, it can be helpful to find recent publications using a similar experimental design, or to speak with experts at a sequencing core.

Because sequencing data and applications are so diverse, there is no one-size-fits-all solution for quality control.
Therefore, it is important to think critically about your expectations, and what patterns you expect to see given your data and your biological problem.

#### Scale workflows with tools that leverage computational approximations

Many bioinformatics workflows take a long time and significant computational resources to run, and interpretable results are often only produced by the last few steps.
This means that time-to-insight from sequencing data is often very high.

Understanding the basic structure of data, the relationship between samples, and the approximate composition of each sample is very helpful at the beginning of data analysis, and can often drive analysis decisions in different directions than those originally intended.
Although most bioinformatics workflows generate these types of insights, there are a few tools that do so rapidly, allowing the user to generate quick hypotheses that can be further tested by more extensive, fine-grained analyses.

**Sketching** Sketching algorithms work with compressed approximate representations of sequencing data and thereby reduce runtimes and computational resources.
These approximate representations retain enough information about the original sequence to recapitulate the main findings from many exact but computationally intensive workflows.
Most sketching algorithms estimate sequence similarity in some way, allowing the user to gain insights from these comparisons.
For example, sketching algorithms can be used to estimate all-by-all sample similarity which can be visualized as a Principle Component Analysis or a multidimensional scaling plot, or can be used to build a phylogenetic tree with accurate topology.
Sketching algorithms also dramatically reduce the runtime for comparisons against databases (e.g. all of GenBank), allowing users to quickly compare their data against large public databases.
Sketching algorithms have been reviewed in-depth by Rowe [@doi:10.1186/s40168-019-0653-2].

**Read quasi-mapping vs alignment** RNA-seq analysis approaches like differential expression or transcript clustering rely on transcript or gene counts.
Many tools can be used to generate these counts by quantifying the number of reads that overlap with each transcript or gene.
For example, tools like STAR and HISAT2 produce alignments that can be post-processed to generate per-transcript read counts [@doi:10.1093/bioinformatics/bts635; @doi:10.1038/s41587-019-0201-4].
However, these tools generate information-rich output, specifying per-base alignments for each read.
Quasi-mapping produces the minimum information necessary for read quantification, thereby reducing the time and resources needed to generate and store read count information [@doi:10.1093/bioinformatics/btw277].

discuss blast approximations?






## lost bits. etc

We briefly discuss tools for quickly gaining insights from large datasets that are becoming increasingly useful as the scale of available data increases.

 and storage recommendations that will facilitate biological workflows.

Then, we discuss tools for quickly gaining insights from large datasets and practical strategies for saving yourself valuable time and resources.

As resource management becomes especially important with the increasing size of analysis datasets,

As our capacity to analyze larger datasets increases, diligent resource management becomes critical.

pay dividends
save time.

- more data is not always better
- well-designed/ thought out analysis will save you time, energy,. money, resources
-
With increased sequencing capacity, careful experimental design and data acquisition

However, as data scales up, proper/well thought-out/well-designed/ data selection/acquisition and diligent resource management becomes especially important.

careful/directed/ conscientious/ scrupulous
methodical resource management
diligent resource management

below ... data management considerations
... tips and tools for saving yourself valuable time and resources as your workflows and analysis scale up
scaling up your workflows
..save yourself valuable time and resources.

Below, we discuss practical considerations for biological sequence analysis that


Careful, measured, documented data management is thus critical to workflow-enabled biology

data and resource management is critical.

As datasets scale up

with workflows, your capacity to analyze a lot of data at once will increase..

data management = similar to previous advice, with a few things slanted towards workflows
As datasets scale up, resource management becomes especially important

While workflows enable data intensive biology, many steps like finding or backing up data are orchestrated outside of a workflow.
Below we discuss practical considerations for working with large-scale data sets.
