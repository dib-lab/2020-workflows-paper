## Practical considerations for workflow-enabled biology

While workflows enable data intensive biology, many steps like finding or backing up data are orchestrated outside of a workflow.
Below we discuss practical considerations for working with large-scale data sets.

### Obtaining data

As with all biological analyses, a critical step of sequence analysis is obtaining high-quality data for your scientific question.
With vast amounts of sequencing data already available in public repositories, it is often possible to begin investigating your research question by seeking out publicly available data.
In some cases, these data will be sufficient to conduct your entire analysis.
In others cases, particularly for biologists conducting novel experiments, these data can inform decisions about sequencing type, depth, and replication, and can help uncover potential pitfalls before they cost valuable time and resources.

### Accessing publicly-available data

Most journals now require data for all manuscripts to be made accessible, either at publication or after a short moratorium.
You can find relevant sequencing data either by starting from the "data accessibility" sections of papers relevant to your research or by directly searching for your organism, environment, or treatment of choice in public data portals and repositories.

The International Nucleotide Sequence Database Collaboration (INSDC), which includes the Sequence Read Archive (SRA), European Nucleotide Archive (ENA), and DataBank of Japan (DDBJ) is the largest repository for raw sequencing data [@doi:10.1093/nar/gkv1323].
Additional curated databases focus on processed data instead, such as gene expression in the Gene Expression Omnibus (GEO) [@doi:10.1093/nar/30.1.207].
Organism-specific databases such as **Wormbase** (*Caenorhabditis elegas*) specialize on curating and integrating sequencing and other data associated with a model organism [@doi:10.1093/nar/gkz920].
The SRA, ENA, and DDBJ no longer accept raw sequencing data from large consortia projects, so data from these efforts are often hosted in consortia-specific databases such as those hosted by the Tara Ocean Foundation [@doi:10.1038/sdata.2015.23].
Unlike the SRA and associated databases which are centralized and searchable, databases overseen by consortia often require domain-specific knowledge and have unique download and authentication protocols.
Finally, rather than focusing on certain data types or organisms, some repositories are designed to hold any data and metadata associated with a specific project or manuscript (e.g. Open Science Framework, Dryad, Zenodo [@doi:10.5195/JMLA.2017.88]).

### Generating your own data

If generating your own data, proper experimental design and planning are essential.
For cost-intensive sequencing data, there are a range of decisions about experimental design and sequencing (including sequencing type, sequencing depth per sample, and biological replication) that impact your ability to properly address your research question.
These considerations will be different for different types of sequence analysis.
While we have curated a series of domain-specific references that may be useful as go about designing your experiment (see **Table @tbl:seq_resources**), conducting discussions with experienced bioinformaticians and statisticians, **prior to beginning your experiments** if possible, is the best way to ensure you will have sufficient statistical power to detect effects.
Given the resources invested in collecting samples for sequencing, it's important to build in a buffer to preserve your experimental design in the face of unexpected laboratory or technical issues.


|  Sequencing type | Resources |
| --- | --- |
|  RNA-sequencing | [@doi:10.1186/s13059-016-0881-8; @doi:10.1261/rna.058339.116; @doi:10.1261/rna.046011.114] |
|  Metagenomic sequencing | [@doi:10.1038/nbt.2235; @doi:10.1038/nbt.393; @doi:10.1016/j.tim.2018.11.003]|
|  Amplicon sequencing | [@doi:10.7554/eLife.46923; @doi:10.1371/journal.pone.0124671; @doi:10.1038/nbt.3981] |
|  Microbial isolate sequencing | [@doi:10.1038/srep08747] |
|  Eurkaryotic genome sequencing |  |
|  Whole-genome resequencing | [@doi:10.1111/mec.14264] |
|  Rad seq |  |
|  Chip seq |  |
|  ATAC seq |  |
|  single cell RNA-seq | [@doi:10.1186/s13059-016-0927-y; @doi10.1186/s13073-017-0467-4] |
|  ? |  |
Table: References for experimental design and considerations for common sequencing chemistries. {#tbl:seq_resources}


As your experiment progresses, keep track of as much information as possible -- dates and times of sample collection, storage, and extraction, sample names, aberrations that occurred during collection, kit lot used for extraction, and any other sample measurements you might be able to obtain (temperature, location, metabolite concentration, name of collector, etc).
This metadata allows you to keep track of your samples, to control for batch effects that may arise from unintended batching during sampling or experimental procedures and makes the data you collect reusable for future applications and analysis by yourself and others.
When working with metadata, using a standardized format that is easy for a computer to read can simplify downstream analysis.
Standard guidelines for formatting data for scientific computing are given in [@doi:10.1371/journal.pcbi.1005510].

### Securing Computational Resources

Sequence analysis requires access to computing systems with adequate storage and analysis power for your data.
For some smaller-scale datasets, local desktop or even laptop systems can be sufficient, especially if using tools that implement data-reduction strategies such as minhashing [@doi:10.1186/s40168-019-0653-2].
However, larger projects require additional computing power, or may be restricted to certain operating systems (e.g. linux).
For these projects, solutions range from research-focused high performance computing systems to research-integrated commercial analysis platforms.
Both research-only and  and commercial clusters provide avenues for research and educational proposals to enable access to their computing resources (see **Table @tbl:computational_resources**).
In preparing for data analysis, be sure to allocate sufficient computational resources and funding for storage and analysis, including large intermediate files and resources required for personnel training.

|  Cloud Provider | Standard Model | Limits |
| --- | --- | --- |
|  Amazon Web Services | Paid |  |
|  Bionimbus Protected Data Cloud | Research allocation | users with eRA commons account |
|  Cyverse Atmosphere | Free with limits | storage and compute hours |
|  EGI federated cloud | Access by contact | European partner countries |
|  Galaxy | Free with storage limits | data storage limits |
|  Google Cloud Platform | Paid |  |
|  Google Colab | Free | computational notebooks, no resource guaruntees |
|  Microsoft Azure | Paid |  |
|  NSF XSEDE | Research allocation | USA researchers or collaborators |
|  Open Science Data Cloud | Research allocation |  |
|  Wasabi | Paid | data storage solution only |
Table: **Research cloud resources** Cloud provider indicates the name of the cloud, standard model indicates the most common route toward using the cloud, and limitations indicates limitations in access or services provided by the cloud.{#tbl:computational_resources}

### Transferring Data (or not)

If you're working with publicly-available data, you may be able to work on a compute system where the data are already available, circumventing time and effort required for downloading and moving the data.
Databases such as the Sequence Read Archive (SRA) are now available on commercial cloud computing systems, and open source projects such as Galaxy enable import and work on SRA sequence files directly from a web browser [@doi:10.1093/nar/gky379; @url:https://www.ncbi.nlm.nih.gov/sra/docs/sra-cloud/].
Ongoing projects such as the NIH Common Fund Data Ecosystem, aim to develop a data portal to make NIH Common Fund data, including biomedical sequencing data, more findable, accessible, interoperable, and reusable (FAIR).

In most cases, you'll still need to transfer some data - either downloading raw data or transferring important intermediate and results files for backup and sharing (or both).
Checksums can be used to to check file integrity and ensure proper transfer (see **Figure @fig:checksum**).
File compression (gzip, bzip2, BAM/CRAM, etc.) can improve transfer speed and save space.
Tools like Rsync and Rclone automate file transfer between computers or between computers and remote storage providers [@doi:10.1111/2041-210X.12550].
These tools automatically use checksums to verify that files were transferred properly.
Some GUI file transfer tools (e.g. cyberduck) also assess checksums when they are provided.


![**Use Checksums to ensure file integrity** Checksum programs (e.g. md5, sha256) encode file size in a single value known as a "checksum". For any file, this value will be identical across platforms when calculated using the same checksum program. When transferring files, calculate the value of the checksum prior to transfer, and then again after transfer. If the value is not identical, there was an error introduced during transfer (e.g. file truncation, etc).
For publicly-available files, a checksum value is often provided, so that you can check the integrity of the file after download.](images/checksum.pdf){#fig:checksum}


### Backup and storage

Workflow sysems allow users to generate an automatable, repeatable workflow.
This removes the necessity of storing some intermediate files as these can be easily regenerated.
However, storing or backing up raw data or computationally intensive results can be a burden in data intensive workflows.
Many universities provide cloud storage space (e.g. through google drive, box, dropbox etc), and researchers can pay for individual storage on these services, or services attached to cloud computing (e.g. Amazon Web Services).
Full computer backups can be conducted to these storage locations (e.g. with rclone [@doi:10.1111/2041-210X.12550]), or there are also a number of paid services that will conduct backups at regular intervals (e.g. Backblaze, Dropbox Pro).
Free online repositories mentioned in the version control section like OSF, Dryad, and Zenodo can also store or backup important files.

Regardless of which storage or backup strategy you choose, it is a good idea to have multiple independent backups of raw data and workflows; these cannot be easily regenerated if lost to a computer failure or other unforseeable event like a lab fire.

% Link out to data/project management info? e.g. Many of these considerations are addressed in a data carpentry lesson (https://datacarpentry.org/organization-genomics/).

### Ensuring you have quality data

The adage, "garbage in, garbage out" describes most sequencing data analysis: the quality of the input data determines the quality of the output results.
This is true whether your workflow analyzes six samples or 600 samples.
Assessing data at every analysis step can reveal problems and errors early.
You are the single most effective quality control tool that you have, so its important to interrogate your data to search for problems.
While simply looking at your data is sometimes sufficient to catch issues, visualization and software tools make quality control easier.

### Critically assess your data

Quality control can be as simple as looking at the first few and last few lines of input and output data files, or checking the size of those files (see **Table @tbl:bash_commands**).
To develop an intuition for what proper inputs and outputs look like for a given tool, it is often helpful to first run the test example or data that is packaged with the software.
Comparing these input and output file formats to your own data can help identify and address inconsistencies.

Visualization is another powerful way to pick out unusual or unexpected patterns.
Although large abnormalities may be clear from looking at files, others may be small and difficult to find.
Visualizing raw sequencing data with FastQC (**Figure {@fig:multiqc}B**) and processed sequencing data with tools like the Integrative Genome Viewer and plotting tabular results files using python or R can make aberrant or inconsistent results easier to track down [@url:https://www.bioinformatics.babraham.ac.uk/projects/fastqc/; @doi:10.1093/bib/bbs017].

Many tools generate log files or messages while running.
These files contain information about the quantity, quality, and results from the run, or error messages about why a run failed.
Inspecting these files can be helpful to make sure tools ran properly and consistently, or to debug failed runs.
Parsing and visualizing log files with a tool like MultiQC can improve interpretability of program-specific log files (**Figure @fig:multiqc** [@doi:10.1093/bioinformatics/btw354]).


|  command | function | example |
| --- | --- | --- |
|  ls -lh | list files with information in a human-readable format | ls -lh \*fastq.gz |
|  head | print the first 6 lines of a file to standard out | head samples.csv |
|  tail | print the last 6 lines of a file to standard out | tail samples.csv |
|  less | show the contents of a file in a scrollable screen | less samples.csv |
|  zless | show the contents of a gzipped file in a scrollable screen | zless sample1.fastq.gz |
|  wc -l | count the number of lines in a file | wc -l ecoli.fasta |
|  cat | print a file to standard out | cat samples.csv |
|  grep | find matching text and print the line to standard out | grep ">" ecoli.fasta |
|  cut | cut columns from a table | cut -d"," -f1 samples.csv |
Table: Some bash commands are useful to quickly explore the contents of a file. By using these commands, the user can detect common formatting problems or other abnormalities. {#tbl:bash_commands}


![**Visualizations produced by MultiQC.** 
**A** MultiQC summary of FastQC Per Sequence GC Content for 1905 metagenome samples. FastQC provides quality control measurements and visualizations for raw sequencing data, and is a near-universal first step in sequencing data analysis because of the insights it provides  [@url:https://www.bioinformatics.babraham.ac.uk/projects/fastqc/; @doi:10.1093/bib/bbs017]. 
FastQC measures and summarizes 10 quality metrics and provides recommendations for whether the sample is within an acceptable quality range. 
Not all metrics readily apply to all sequencing data types. For example, while multiple GC peaks might be concerning in whole genome sequencing of a bacterial isolate, we would expect a non-normal distrubtion for some metagenome samples that contain organisms with diverse GC content. 
Samples like this can be seen in red in this figure. 
**B** MultiQC summary of Salmon *quant* reads mapped per sample for RNA-seq samples []@doi:10.1038/nmeth.4197]. MultiQC finds and automatically parses log files from other tools and generates a combined report and parsed data tables that include all samples. MultiQC currently supports 88 tools. In this figure, we see that MultiQC summarizes the number of reads mapped and percent of reads mapped, two values that are reported in the Salmon log files.
](images/multiqc.pdf){#fig:multiqc}


### Handling common biases in sequencing data

Biases in sequencing data originate from experimental design, methodology, sequencing chemistry, or workflows, and are helpful to target specifically with quality control measures.
For example, PCR duplicates can cause problems in libraries that underwent an amplification step, and often need to be removed prior to downstream analysis [@doi:10.1038/nrg3788; @doi:10.1038/srep25533; @doi:10.1086/BBLv227n2p146; @doi:10.1186/s12864-018-4933-1; @doi:10.1186/s13059-014-0420-4].
Contamination can arise during sample collection, nucleotide extraction, library prepartion, or through sequencing spike-ins like PhiX, and could change data interpretation if not removed [@doi:10.1073/pnas.1510461112; @doi:10.1073/pnas.1600338113; @doi:10.1186/1944-3277-10-18].
Libraries sequenced with high concentrations of free adapters or with low concentration samples may have increased barcode hopping, leading to contamination between samples [@doi:10.1111/1755-0998.13009].
Stringent trimming of RNA-sequencing data may reduce isoform discovery [@doi:10.3389/fgene.2014.00013].
The exact biases in a specific data set or workflow will vary greatly between experiments.
To determine what issues are most likely to plague your specific data set, it can be helpful to find recent publications using a similar experimental design, or to speak with experts at a sequencing core.

Because sequencing data and applications are so diverse, there is no one-size-fits-all solution for quality control.
Therefore, it is important to think critically about your expectations, and what patterns you expect to see given your data and your biological problem.
