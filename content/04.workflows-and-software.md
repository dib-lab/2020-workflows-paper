## Workflows facilitate data-intensive biology

Sequence analyses require researchers to build workflows that synthesize multiple analytic tools and execute them in a systematic manner across all experimental samples.
These workflows commonly produce hundreds to thousands of intermediate files and require incremental changes as experimental insights demand tool and parameter modifications.
Managing these steps can be both time-consuming and error-prone, even when automated using scripting languages (e.g. bash).

The emergence and maturation of workflow systems designed with bioinformatic challenges in mind has revolutionized computing in data intensive biology [@doi:10.1038/s41592-018-0046-7].
Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workflow (**Figure @fig:workflow**).
These features ensure that the steps for data analysis are documented and repeatable from start to finish.
When paired with proper software management, fully-contained workflows are scaleable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user in weeks, months, or years from the time of writing.

![**Workflow Systems** Bioinformatic workflow systems have built-in functionality that facilitate/streamline/simplify running analysis pipelines.
**A. Samples** Workflow systems enable you to use the same code to run each step on each sample. Samples can be easily added if the analysis expands.
**B. Software Management** Integration with software management tools (e.g. conda, singularity) can automate software installation for each step.
**C. Branching, F. Ordering, G. Parallelization** Workflow systems ensure
tasks are executed in the correct order for each sample file, and can automatically execute independent steps in parallel.
**D.Standard Steps** Many steps are now considered "standard" (e.g. quality control). Workflow languages keep all information for a step together and can be written to enable you to remix and reuse each individual step across pipelines.
**E. Rerun as necessary** Workflow systems keep track of which steps executed properly and on which samples, and allow you to rerun failed steps (or additional steps) rather than re-executing the entire workflow.
**H. Reporting** Workflow languages enable comprehensive reporting on workflow execution and resource utilization by each tool.
**I. Portability** Analyses written in workflow languages (with integrated software management) can be run across computing systems without changes to code.](images/workflow_figure.svg){#fig:workflow}

In order to properly direct a workflow, workflow systems need to encode information about the relationships between workflow steps.
In practice, this means that each analysis step must specify the input (or types of inputs) needed for that step, and the output (or types of outputs) being produced.
This structure results in several added benefits, beyond those mentioned above.
First, workflows become self-documented; the directed graph produced by workflow systems can be exported and visualized, producing a graphical representation of the relationships between all steps in a pipeline (see **Figure @fig:sgc_workflow**).
Next, workflows are more likely to be fully enclosed without undocumented steps that are executed by hand, meaning analyses are more likely to be reproducible.
Finally, each step becomes a self-contained unit that can be used and re-used across multiple analysis workflows, so scientists can spend less time implementing standard steps, and more time on their specific research questions.
In sum, the internal scaffolding provided by workflow systems helps build analyses that are generally better documented, repeatable, transferable, and scalable.


#### Choosing a workflow system
While the benefits of encoding a workflow in a workflow system are immense, the learning curve associated with implementing complete workflows in a new syntax can be daunting.
It is possible to obtain the benefits of workflow systems without learning a workflow system.
Websites like Galaxy, Cavatica, and EMBL-EBI MGnify offer online portals in which users build workflows around publicly-available or user-uploaded data [@doi:10.1093/nar/gky379; @doi:10.14694/EDBK_175029; @doi:10.1093/nar/gkz1035].
On the command line, many research groups have used workflow systems to build user-friendly pipelines that do not require learning or working with the underlying workflow software.
These tools are specified in an underlying workflow language, but are packaged in a more user-friendly manner command-line script that coordinates and executes the workflow.
Rather than writing each workflow step, the user can specify data and parameters in a configuration file to customize the run.
Some examples include the nf-core RNA-seq pipeline [@url:https://github.com/nf-core/rnaseq/; @doi:10.1038/s41587-020-0439-x], the ATLAS metagenome assembly and binning pipeline [@url:https://github.com/metagenome-atlas/atlas; @doi:10.1101/737528], the Sunbeam metagenome analysis pipeline [@url:https://github.com/sunbeam-labs/sunbeam; @doi:10.1186/s40168-019-0658-x], and two from our own lab, the Elvers *de novo* transcriptome and differential expression pipeline [@url:https://github.com/dib-lab/elvers], and dammit eukaryotic transcriptome annotation pipeline [@url:https://github.com/dib-lab/dammit].
These tools allow users to take advantage of the benefits of workflow software without needing to invest in curating and writing their own pipeline. They are designed to execute a series of standard steps, and provide varying degrees of customizability.

While these systems can be incredibly powerful and versatile, bioinformatic analyses are highly dependent on experimental design, and researchers often need to build custom workflows or steps for their analyses.
At this time, there are several scriptable workflow systems that offer similar benefits for data intensive biology.
Each has it own strengths, meaning each software will meet an individuals computing goals differently (see **Table @tbl:workflows**).
Our lab has adopted Snakemake, in part due to its similarity and integration with Python, its flexibility for building and testing new analyses in different languages, and its intuitive integration with software management tools (SEE SECTION XXX)[@doi:10.1093/bioinformatics/bts480].
Software like Nextflow and Common Workflow Language require slightly more infrastructure, but in return, scale much better to pipelines with hundreds of thousands of steps and support containerization more rigidly, making them ideal for production-level pipelines [@doi:10.1038/nbt.3820; @doi:10.6084/m9.figshare.3115156.v2].
Language-specific workflow systems, such as ROpenSci's Drake [@doi:10.21105/joss.00550], are limited in the scope of tasks they can execute, but are powerful within their language and easier to integrate if comfortable in that language.


|Workflow System | Documentation | Example Workflow | Tutorial |
|----------------|---------------|------------------|------------------|
| Snakemake | https://snakemake.readthedocs.io/ | https://github.com/snakemake-workflows/chipseq | https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html |
| Nextflow | https://www.nextflow.io/ | https://github.com/nf-core/sarek | https://www.nextflow.io/docs/latest/getstarted.html |
| Common workflow language | https://www.commonwl.org/ | https://github.com/EBI-Metagenomics/pipeline-v5 | https://www.commonwl.org/user_guide/02-1st-example/index.html |
| Workflow description language | https://openwdl.org/ | https://github.com/gatk-workflows/gatk4-data-processing |  https://support.terra.bio/hc/en-us/articles/360037127992--1-howto-Write-your-first-WDL-script-running-GATK-HaplotypeCaller |

Table: Popular bioinformatics workflow systems, documentation, example workflows, and tutorials.
While we have linked to general tutorials, there may be more relevant tutorials online for your field.
Not all workflow systems are not necessarily exclusive entities: some workflow systems can translate workflows between languages or run tools or tasks written in other languages.
{#tbl:workflows}


The best workflow system to adopt may be the one with a strong and accessible local or online community in your field, somewhat independent of your computational needs.
The availability of field-specific data analysis code for reuse and modification, as well as community support for new users can facilitate the adoption process.
Fortunately, the standardized syntax required by workflow systems, combined with widespread adoption in the open science community, has resulted in a proliferation of open access workflow-system code for routine analysis steps [@doi:10.1007/978-1-4939-9074-0_24, @doi:10.1038/s41587-020-0439-x; @doi:10.21105/joss.00352].
At the same time, consensus approaches for data analysis are emerging, further encouraging reuse of existing code [@doi:10.1186/s13059-016-0881-8; @doi:10.1038/nbt.3935; @doi:10.15252/msb.20188746; @doi:10.1016/j.margen.2016.04.012; @doi:10.1038/s41579-018-0029-9].

#### Getting started with workflows

The first place to start with any workflow language is the tutorials and available code. We have linked some examples in **Table {@tbl:workflows}**, but you may be able to find tutorials that are specific to your analysis or field. After working through at least one tutorial, you can move to running example workflows that somewhat closely resemble the tool or task you need to accomplish. Read through the script with the system documentation on hand in order to make sense of the structure. If available, run the test data to see what the output will look like. The "Getting started developing workflows" section will contain strategies for developing and modifying workflows for your own analyses.

### Wrangling Scientific Software

Analysis workflows rely on multiple software packages to generate final results.
These tools are heterogeneous in nature: written by researchers working in different coding languages, with varying types software design and optimization, and often for specific analysis goals.
Each program has a number of other programs it depends upon to function ("dependencies"), and as software changes over time to meet research needs, the results may change, even when run with identical parameters.
As a result, it is critical to take an organized approach to installing, managing, and keeping track of software and software versions.
To meet this need, most workflow managers integrate with software management systems like conda, singularity, and docker [@url:https://pyvideo.org/pycon-au-2015/conda-a-cross-platform-python-agnostic-binary-p.html; @doi:10.1371/journal.pone.0177459; @doi:10.5555/2600239.2600241].

Software management systems perform some combination of software installation, management, and packaging that alleviate problems that arise from dependencies and that facilitate documentation of software versions.
On many systems, system-wide software management is overseen by system administrators, who ensure commonly-used and requested software is installed into a "module" system available to all users.
Unfortunately, this system does not lend itself well for exploring new workflows and software, as researchers do not have permission to install software themselves.
The Conda package manager has emerged as a leading software installation and management solution, largely because it handles both cluster permission and version conflict issues with a user-based software environment system, and features a straightforward "recipe" system which simplifies the process of making new software installable (**Figure @fig:conda_figure**).
Conda enables lightweight software installation and can be used with the same commands across platforms.
Alternatively, container solutions like docker and singularity allow for for the entire computational environment to be captured and distributed, including the operating system.
This ensures that an environment is completely reproducible, and is common for production workflows.

![**The conda package and environment manager simplifies software installation and management.**
**A. Conda Recipe Repositories** Each program distributed via Conda has a "recipe" describing all software dependencies needed for Conda installation (each of which must also be installable via conda). These are stored and managed in separate "channels", some of which specialize (e.g. "bioconda" specializes in bioinformatic software, "r" specializes in R language packages) [@doi:10.1038/s41592-018-0046-7]. **B. Use Conda Environments to Avoid Installation Conflicts**  Conda does not require root
privileges for software installation, thus enabling use by researchers working on shared cluster systems. However, even user-based software installation can encounter dependency conflicts. For example, you might need to use python2 to install and run a program (e.g. older scripts written by members of your lab), while also using snakemake to execute your workflows (requires python>=3.5). By installing each program into an isolated "environment" that contains only the software required to run that
program, you can ensure all programs will run without issue. Using small, separate environments for your software and building many simple environments to accommodate different steps in your workflow also reduces the amount of time it takes conda to resolve dependency conflicts between different software tools ("solve" an environment). Conda virtual environments can be created and installed either on the command line, or via an environment YAML file, as shown. In this case, the environment file
also specifies which Conda channels to search and download programs from. When specified in a YAML file, conda environments are easily transferable between computers and operating systems. Further, because the version of each package installed in an environment is recorded, workflow reproducibility is enhanced. Although portions of conda may be superceded by alternative solutions [@url:https://github.com/QuantStack/mamba], this model of software installation and management will likely
remain.](images/conda_figure.svg){#fig:conda_figure height=8in}

#### Getting started with software management

Using these software management tools within workflow systems is relatively straightforward: install the manager if needed, then add a system-specific line to each workflow step that specifies information for software installation. With snakemake, for example, add a link to an environment, system module, or container for the required software to the code for that analysis step (conda environment example in **Figure {@fig:conda_figure}, B**).
If the workflow is then executed with the "--use-conda", "--use-singularity", or "-use-envmodules" options, the specified environment or container will be downloaded and installed (or system module will be loaded), used to run that analysis step, and stored for future analysis runs. The specific syntax for using package managers with your chosen workflow system will be available in the documentation.

While package managers and containers greatly increase reproducibility, there are a number of ways to test software before (or without ever) needing to worry about installation.
Some software packages are available as web-based tools and through a series of data upload and parameter specifications, allow the user to interact with a tool that is running on a back-end server.
This approach is ideal for testing a tool prior to installation to determine whether it produces an appropriate or useful output on your data.
Integrated development environments like PyCharm and RStudio can also manage software installation for language-specific tools.
In our experience, the complete solution for using scientific software involves a combination of conda with these tools for developing new analyses, as well as workflow-integrated software management via conda, singularity, or docker for executing full workflows on many samples.
