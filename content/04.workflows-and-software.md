## Workflows and Software Management

Sequence analyses require researchers to build workflows that synthesize multiple analytic tools and execute them in a systematic manner across all experimental samples.
These workflows commonly produce hundreds to thousands of intermediate files and require incremental changes as experimental insights demand tool and parameter modifications.
Managing these steps can be both time-consuming and error-prone, even when automated using scripting languages (e.g. bash).

The emergence and maturation of workflow systems designed with bioinformatic challenges in mind has revolutionized computing in data intensive biology [@doi:10.1038/s41592-018-0046-7].
These workflow systems,  contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workflow (**Figure @fig:workflow**).
These features ensure that the steps for data analysis are documented and repeatable from start to finish.
When paired with proper software management, fully-contained workflows are scaleable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user in weeks, months, or years from the time of writing.

![**Workflow Systems** Bioinformatic workflow systems have built-in functionality that facilitate/streamline/simplify running analysis pipelines.
**A. Samples** Workflow systems enable you to use the same code to run each step on each sample. Samples can be easily added if the analysis expands.
**B. Software Management** Integration with software management tools (e.g. conda, singularity) can automate software installation for each step.
**C. Branching, F. Ordering, G. Parallelization** Workflow systems ensure
tasks are executed in the correct order for each sample file, and can automatically execute independent steps in parallel.
**D.Standard Steps** Many steps are now considered "standard" (e.g. quality control). Workflow languages keep all information for a step together and can be written to enable you to remix and reuse each individual step across pipelines.
**E. Rerun as necessary** Workflow systems keep track of which steps executed properly and on which samples, and allow you to rerun failed steps (or additional steps) rather than re-executing the entire workflow.
**H. Reporting** Workflow languages enable comprehensive reporting on workflow execution and resource utilization by each tool.
**I. Portability** Analyses written in workflow languages (with integrated software management) can be run across computing systems without changes to code.](images/workflow_figure.svg){#fig:workflow}

In order to properly direct a workflow, workflow systems need to encode information about the relationships between workflow steps.
In practice, this means that each analysis step must specify the input (or types of inputs) needed for that step, and the output (or types of outputs) being produced.
This structure results in several added benefits, beyond those mentioned above.
First, workflows become self-documented; the directed graph produced by workflow systems can be exported and visualized, producing a graphical representation of the relationships between all steps in a pipeline (see **Figure @fig:sgc_workflow**).
Next, workflows are more likely to be fully enclosed without undocumented steps that are executed by hand, meaning analyses are more likely to be reproducible.
Finally, each step becomes a self-contained unit that can be used and re-used across multiple analysis workflows, so scientists can spend less time implementing standard steps, and more time on their specific research questions.
In sum, the internal scaffolding provided by workflow systems results in analyses that are generally better documented, repeatable, transferable, and scalable.


**paragraph: choosing a system/language**
While the benefits of encoding a workflow in a workflow system are immense, the learning curve associated with implementing complete workflows in a new syntax can be daunting.


**paragraph: getting started with your chosen system (start form working code, etc)**

**paragraph: resources and open science stuff?**


workflows must be written in system-specific workflow languages

In order for workflow systems generate directed graphs to execute a workflow,


A critical mass of workflow system-specific code has accumulated such that many routine tasks are already encoded and available for others to use [@doi:10.1038/s41587-020-0439-x; @doi:10.21105/joss.00352].
At the same time, consensus approaches for routine tasks have emerged, further encouraging reuse of existing code [@doi:10.1186/s13059-016-0881-8; @doi:10.1038/nbt.3935; @doi:10.15252/msb.20188746; @doi:10.1016/j.margen.2016.04.012; @doi:10.1038/s41579-018-0029-9].

Advances in workflow systems have led to wide-spread community adoption in part attributable to the open science movement [@doi:10.1007/978-1-4939-9074-0_24].


Several ...
Finally, the

The standard syntax used to specify each step in the workflow lends itself to easy reuse in future projects.
render


These built-in attributes alleviate the overhead of needing to implement substantial scaffolding around core commands on a per-workflow basis.



Workflow systems work by requiring the user to include information about required input files and output files that will be produced by a step within standardized system-specific syntax. The system can then handle workflow ordering and parallelization.
An important intended consequence for this standard syntax is that code is easily reusable remixable across analyses with minimal to no modification.

for running each step, which usually require some information about required input files, the type of output, and of course, the command for running the actual step.



The system can then represent these steps as a directed graph in which each node is a step in the workflow, and edges connect sequential steps.






, often written in different programming languages,

This back-end organization, paired with additional scaffolding, makes workflows

**Workflows systems have emerged as the workhorses of modern bioinformatics.**
end with this sentence? or maybe put in intro/conclusion/etc


### Why are workflows now useful? what has changed? (aka why were people not using them before)

The need for workflow management systems has increased with the plummeting cost of sequencing and availability of public data.
While workflows are ubiquitous in bioinformatics and scientific computing in general, workflow systems designed by bioinformaticians for bioinformatic tasks are relatively new.
Prior to the development of robust workflow systems, common tools for scripting a workflow included make, pydoit, or bash scripting.
These systems required the user to implement substantial scaffolding around core commands on a per-workflow basis.
Workflow systems have alleviated this overhead and simplified the process of scripting a workflow.

### How will your life be changed with workflows?

When a workflow is specified in this way, each step is executed in the proper order and will be rerun if a failure occurs.
This frees the user from manually keeping track of execution and monitoring of each step.
Similarly, the standard syntax used to specify each step in the workflow lends itself to easy reuse in future project.
This is in part enabled by cross-system compatibility of most workflow systems, which allows users to develop a workflow e.g. locally, and scale it on a cluster or a cloud computer.



### Getting the benefits without having to learn a scriptable workflow system

While the benefits of encoding a workflow in a workflow system are immense, the learning curve associated with implementing complete workflows in a new syntax can be daunting.
It is possible to obtain the benefits of workflow systems without learning a workflow software.

Many research groups have used workflow software to build user-friendly pipelines that do not require learning or working with the underlying workflow software.
These tools allow users to take advantage of the benefits of workflow software without needing to invest in curating and writing their own pipeline.
These tools are specified in an underlying workflow language, but the user interacts with a command-line script and configuration file that coordinate and execute the workflow.
Often times, workflow parameters are exposed to the user, allowing the user to control certain behaviors such as parallelization or "dry-runs" when executing the command-line script.
Some examples include the ATLAS metagenome assembly and binning pipeline [@url:https://github.com/metagenome-atlas/atlas; @doi:10.1101/737528], the Sunbeam metagenome analysis pipeline [@url:https://github.com/sunbeam-labs/sunbeam; @doi:10.1186/s40168-019-0658-x], the Elvers *de novo* transcriptome and differential expression pipeline [@url:https://github.com/dib-lab/elvers], the dammit eukaryotic transcriptome pipeline [@url:https://github.com/dib-lab/dammit], and the nf-core RNA-seq pipeline [@url:https://github.com/nf-core/rnaseq/; @doi:10.1038/s41587-020-0439-x].

Workflow systems are also available as graphical user interface systems.
Websites like Galaxy, Cavatica, and EMBL-EBI MGnify offer online portals in which users build workflows around publicly-available or user-uploaded data [@doi:10.1093/nar/gky379; @doi:10.14694/EDBK_175029; @doi:10.1093/nar/gkz1035].

### How to learn to use workflows systems?

There are many scriptable workflow systems that offer similar benefits for data intensive biology.
Given the plethora of choices and the steep learning curve associated with integrating a workflow system into daily workflow management, it can be difficult to decide which workflow system to adopt.
While there are many workflow softwares to choose from, each software has it own strengths, meaning each software will meet an individuals computing goals differently (see **Table @tbl:workflows**).
Our lab has adopted Snakemake given its integration with Python and its flexibility to execute code with different languages (e.g. bash and R) and software management tools (SEE SECTION XXX) [@doi:10.1093/bioinformatics/bts480].
Software like Nextflow and Common Workflow Language scale better to pipelines with hundreds of thousands of steps and support containerization more rigidly, making them ideal for production-level pipelines [@doi:10.1038/nbt.3820; @doi:10.6084/m9.figshare.3115156.v2].
There are also language-specific workflow managers, such as ROpenSci's Drake for R [@doi:10.21105/joss.00550].
Further, workflow systems are not necessarily exclusive entities; Snakemake can export pipelines in Common Workflow Language, allowing the same workflow to be fully compatible with two separate workflow systems.


|Workflow System | Documentation | Example Workflow |
|----------------|---------------|------------------|
| Snakemake | https://snakemake.readthedocs.io/ | https://github.com/snakemake-workflows/chipseq |
| Nextflow | https://www.nextflow.io/ | https://github.com/nf-core/sarek|
| common workflow language | https://www.commonwl.org/ | https://github.com/EBI-Metagenomics/pipeline-v5 |
| workflow definition language | https://openwdl.org/ | https://github.com/gatk-workflows/gatk4-data-processing |
Table: Popular bioinformatics workflow systems, documentation, and example workflows. {#tbl:workflows}


Independent of computational needs, selecting a workflow system with a strong local or online community can facilitate the adoption process.
These communities can provide support for new users, and have likely generated many open and accessible workflows that can be modified to analyze new data.

### Alternatives to workflow systems

Workflow systems are not the only option for constructing and executing a workflow.
Workflow automation can be conducted by scripting the ordered execution of each step in a language such as bash.
While command line scripting is an effective solution to coordinate and execute a workflow, workflows automated in this way do not take advantage of the built-in infrastructure in workflow systems.
In our experience, it is more difficult to identify partially-completed workflow steps that produced truncated files, to rerun specific steps in a workflow, and to add additional files to a workflow when using bash scripting.
These shortcomings are magnified when executing workflows on large-scale sequencing datasets.


### Wrangling Scientific Software

Most workflows rely on multiple software packages to generate final results.
Bioinformatics research software is heterogeneous, where tools are written by different research groups, in different languages, and with varied target audiences.
Each program has a number of other programs it depends upon to function ("dependencies"), and as software changes over time to meet research needs, the results may change, even when run with identical parameters.
As a result, it is critical to take an organized approach to installing, managing, and keeping track of software and software versions.
To meet this need, most workflow managers integrate with software management systems like conda, singularity, and docker [@url:https://pyvideo.org/pycon-au-2015/conda-a-cross-platform-python-agnostic-binary-p.html; @doi:10.1371/journal.pone.0177459; @doi:10.5555/2600239.2600241].

Software management systems perform some combination of software installation, management, and packaging that alleviate problems that arise from dependencies and that facilitate documentation of software versions.
Conda has a emerged as the leading software installation and management solution (**Figure @fig:conda_figure**).
Conda is a package management and environment management system that allows users to install and organize software.
By enabling installation of software from many languages and disciplines, and managing dependency needs and conflicts for those installations, conda brought about a revolution in package management.
Although portions of conda may eventually be superceded by alternative solutions [@url:https://github.com/QuantStack/mamba], the model of software installation and management established by conda will likely remain.
Alternatively, container solutions like docker and singularity allow for for the entire computational environment to be captured and distributed, including the operating systems.
This ensures that an environment is completely reproducible across different computer systems, and is common for production workflows.

While package managers and containers greatly increase reprodubility, there are a number of ways to test out software without needing to worry about installation.
Some software packages are available as web-based tools and through a series of data upload and parameter specifications, allow the user to interact with a tool that is running on a back-end server.
This approach is ideal for testing a tool prior to installation to determine whether it produces an appropriate or useful output on your data.
Integrated development environments like PyCharm and RStudio can also manage software installation for the user for language-specific tools.

![**The conda package and environment manager simplifies software installation and management.**
**A. Conda Recipe Repositories** Each program distributed via Conda has a "recipe" describing all software dependencies needed for Conda installation (each of which must also be installable via conda). These are stored and managed in separate "channels", some of which specialize (e.g. "bioconda" specializes in bioinformatic software, "r" specializes in R language packages) [@doi:10.1038/s41592-018-0046-7]. **B. Use Conda Environments to Avoid Installation Conflicts**  Conda does not require root
privileges for software installation, thus enabling use by researchers working on shared cluster systems. However, even user-based software installation can encounter dependency conflicts. For example, you might need to use python2 to install and run a program (e.g. older scripts written by members of your lab), while also using snakemake to execute your workflows (requires python3.5). By installing each program into an isolated "environment" that contains only the software required to run that
program, you can ensure all programs will run without issue. Using small, separate environments for your software and building many simple environments to accommodate different steps in your workflow also reduces the amount of time it takes conda to resolve dependency conflicts between different software tools ("solve" an environment). Conda virtual environments can be created and installed either on the command line, or via an environment YAML file, as shown. In this case, the environment file
also specifies which Conda channels to search and download programs from. When specified in a YAML file, conda environments are easily transferable between computers and operating systems. Further, because the version of each package installed in an environment is recorded, workflow reproducibility is enhanced.](images/conda_figure.svg){#fig:conda_figure}
