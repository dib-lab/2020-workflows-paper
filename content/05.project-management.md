## Workflow-Based Project Management

Project management, the strategies and decisions used to keep a project organized, documented, functional, and shareable, is foundational to any research program.
Clear organization and management is a learned skill that takes time to implement.
Workflow systems both simplify and improve computational project management, but even workflows that are fully specified in workflow systems require additional investment to stay organized, documented, and backed up. 

### Systematically document your workflows

Pervasive documentation provides indispensable context for biological insights derived from an analysis, facilitates transparency in research, and increases reusability of the analysis code.
Good documentation covers all aspects of a project, including file and results organization, clear and commented code, and accompanying explanatory documents for design decisions and metadata.
Workflow systems facilitate building this documentation, as each analysis step (with chosen parameters) and the links between those steps are completely specified within workflow syntax.
This feature streamlines code documentation, particularly if you include as much of the analysis as possible within the automated workflow framework.
Outside of the analysis itself, applying consistent organizational design can capitalize on the structure and automation provided by workflows to simplify the generation of quality documentation for all aspects of your project.
Below, we discuss project management strategies for building reproducible workflow-enabled biological analyses.

#### Use consistent, self-documenting names

Using consistent and descriptive identifiers for your files, scripts, variables, workflows, projects, and even manuscripts helps keep your projects organized and interpretable for yourself and collaborators.
For workflow systems, this strategy can be implemented by tagging output files with a descriptive identifier for each analysis step, either in the filename or by placing output files within a descriptive output folder.
For example, the file shown in **Figure {@fig:filenaming}** has been preprocessed with a quality control trimming step.
For large workflows, placing results from each step of your analysis in isolated, descriptive folders can be essential for keeping your project workspace clean and organized.

![Consistent and informative file naming improves organization and interpretability. It is useful to keep unique sample identification in the filename, often with a metadata file explaining the meaning of each unique descriptor. For analysis scripts, it can help to implement a numbering scheme, where the name of first file in the analysis begins with "00", the next with "01", etc. For output files, it can help to add a short, unique identifier to output files processed with each analysis step. This particular file is a RAD sequencing fastq file of a fish species that has been preprocessed with a fastq quality trimming tool (courtesy of Shannon Joslin).](images/filenaming.svg){#fig:filenaming}

#### Store workflow metadata with the workflow

Developing biological analysis workflows can involve hundreds of small decisions: What parameters work best for each step?
Why did you use a certain reference file for annotation as compared with other available files?
How did you finally manage to get around the program or installation error?
All of these pieces of information contextualize your results and may be helpful when sharing your findings.
Keeping information about these decisions in an intuitive and easily accessible place helps you find it when you need it.
To capitalize on the utility of version control systems described below, it is most useful to store this information in plain text files.
Each main directory should include notes on the data or scripts contained within, so that a collaborator could look into the directory and understand what to find there (especially since that "collaborator" is likely to be you, a few months from now!).
Code itself can contain documentation - you can include comments with the reasoning behind algorithm choice or include a link to the seqanswers post that helped you decide how to shape your differential expression analysis.
Larger pieces of information can be kept in "README" or notes documents kept alongside your code and other documents.
For example, a GitHub repository documenting the reanalysis of the Marine Microbial Eukaryote Transcriptome Sequencing Project uses a README alongside the code to document the workflow and digital object identifiers for data products [@url:https://github.com/dib-lab/dib-MMETSP; @doi:10.1093/gigascience/giy158].
While this particular strategy cannot be automated, it is critical for interpreting the final results of your workflow.

#### Document data and analysis exploration using computational notebooks

Computational notebooks allow users to combine narrative, code, and code output (e.g. visualizations) in a single location, enabling the user to conduct analysis and visually assess the results in a single file (see **Figure @fig:nb_figure**).
These notebooks allow for fully documented iterative analysis development, and are particularly useful for data exploration and developing visualizations prior to integration into a workflow or as a report generated by a workflow that can be shared with collaborators.

![**Examples of computational notebooks.** Computational notebooks allow the user to mix text, code, and results in one document.
**Panel A** shows an RMarkdown document viewed in the RStudio integrated development environment, while **Panel B** shows a rendered HTML file produced by knitting the RMarkdown document [@url:https://rmarkdown.rstudio.com/].
**Panel C** shows a Jupyter Notebook, where code, text, and results are rendered inline as each code chunk is executed [@https://dx.doi.org/10.3233/978-1-61499-649-1-87].
The second grey chunk is a raw Markdown chunk with text that will be rendered inline when executed.
Both notebooks generate a histogram of a metadata feature, number of generations, from a long-term evolution experiment with *Escherichia coli* [@doi:10.1038/nature18959].
Computational notebooks facilitate sharing by packaging narrative, code, and visualizations together.
Computational notebooks can be further packaged with tools like Binder [@doi:10.25080/Majora-4af1f417-011].
Binder makes a GitHub repository executable by using package management systems and docker to build reproducible and executable software environments as specified in the repository.
Binders can be shared with collaborators (or students in a classroom setting), and analysis and visualization can be ephemerally reproduced or altered from the code provided in computational notebooks.  
](images/nb_figure.png){#fig:nb_figure}


#### Visualize your workflow

Visual representations can help illustrate the connections in a workflow and improve the readability and reproducibility of your project. At the highest level, flowcharts that detail relationships between steps of a workflow can help provide big-picture clarification, especially when the pipeline is complicated.
For individual steps, a graphical representation of the output can show the status of the project or provide insight on additional analyses that should be added. For example, **Figure {@fig:sgc_workflow}** exhibits a modified Snakemake workflow visualization from a recent publication [@doi:10.1101/462788].

![A directed acyclic graph (DAG) that illustrates connections between all steps of a sequencing data analysis workflow. Each box represents a step in the workflow, while lines connect sequential steps.
The DAG shown in this figure illustrates a real bioinformatics workflow from and was generated by modifying the default Snakemake workflow DAG [@doi:10.1101/462788].
The colors represent arms of the workflow that achieve a final result, such as a multiple sequence alignment of a protein of interest.
While the workflow is complex, it is coordinated by a workflow system that alleviates the need for a user to manage file interdependencies.](images/hu_dag.png){#fig:sgc_workflow}

### Version control your project

As your project develops, version control allows you to keep track of changes over time.
You may already do this in some ways, perhaps with frequent hard drive backups or by manually saving different versions of the same file  - e.g. by appending the date to a script name or appending "version_1" or "version_FINAL" to a manuscript draft.
For computational workflows, using version control systems such as Git or Mercurial can be used to keep track of all changes over time, even across multiple systems, scripting languages, and project contributors (see **Figure {@fig:version_control}**).
If a key piece of a workflow inexplicably stops working, good version control can allow you to rewind in time and identify differences from when the pipeline worked to when it stopped working.
Backing up your version controlled analysis in an online repository such as GitHub, GitLab, or Bitbucket provides critical insurance as you iteratively modify and develop your workflow.

![**Version Control** Version control systems (e.g. Git, Mercurial) work by storing incremental differences in files from one saved version ("commit") to the next. To visualize the differences between each version, text editors such as Atom and online services such as GitHub, GitLab and Bitbucket use red highlight to denote deletions, and green highlighting to denote additions. In this trivial example, a typo in version 1 (in red) was corrected (green). These systems are extremely useful for code and manuscript development, as it is possible to return to the snapshot of any saved version. This means that version control systems save you from accidental deletions, preserve code you thought you no longer needed, and preserve a record of project changes over time.](images/version_control.svg){#fig:version_control}

When combined with online backups, version control systems also facilitate code and data availability and reproducibility for publication.
For example, to preserve the version of code that produced published results, you can create a "release", a snapshot of the current code and files in a GitHub repository.
You can then generate a digital object identifier (DOI) for that release using a permanent documentation service such as Zenodo and make it available to reviewers and beyond (see "sharing" section, below).

### Share your workflow and analysis code

Sharing your workflow code with collaborators, peer reviewers, and scientists seeking to use a similar method can foster discussion and review of your analysis.
Sticking to a clear documentation strategy, using a version control system, and packaging your code in notebooks or as a workflow prepare them to be easily shared with others.
To go one step further, you can package your code with tools like Binder and Whole Tale (CTB: reprozip, other such things?), or use Shiny apps. These approaches let others run the code on cloud computers in environments identical to those in which the original computation was performed (**Figure @fig:nb_figure**, **Figure @fig:interactiveviz**) [@doi:10.25080/Majora-4af1f417-011; @doi:10.1016/j.future.2017.12.029].
These tools substantially reduce overhead associated with interacting with code and data, and in doing so, make it fast and easy to rerun portions of the analysis, check accuracy, or even tweak the analysis to produce new results. If you also share your code and workflows publicly, you will also help contribute to the growing resources for open workflow-enabled biological research.

![Interactive visualizations facilitate sharing and repeatability.
**A** Interactive visualization dashboard in the Pavian Shiny app for metagenomic analysis [@url:https://fbreitwieser.shinyapps.io/pavian/; @doi:10.1093/bioinformatics/btz715].
Shiny allows you to build interactive web pages using R code.
Data is manipulated  by R code in real-time in a web page, producing analysis and visualizations of a data set.
Shiny apps can contain user-specifiable parameters, allowing a user to control visualizations or analyses. As seen above, sample "PT1" is selected, and taxonomic ranks class and order are excluded.
Shiny apps allow collaborators who may or may not know R to modify R visualizations to fit their interests.
**B** Plotly heatmap of transcriptional profiling in human brain samples [@url:https://plotly.com/python/v3/ipython-notebooks/bioinformatics/#4-heatmap-of-gene-expression].
Hovering over a cell in the heatmap displays the sample names from the x and y axis, as well as the intensity value.
Plotting tools like plotly and vega-lite produce single interactive plots that can be shared with collaborators or integrated into websites [@url:https://plotly.com/; @doi:10.1109/TVCG.2016.2599030].
Interactive visualizations are also helpful in exploratory data analysis.
](images/interactive_viz.png){#fig:interactiveviz}


### Getting started developing workflows

In our experience, the best way to have your workflow system work _for_ you is to include as much of your analysis as possible within the automated workflow framework, use self-documenting names, include analysis visualizations, and keep rigorous documentation alongside your workflow that enables you to understand each decision and entirely reproduce any manual steps.
Some of the tools discussed above will inevitably change over time, but these principles apply broadly and will help you design clear, well-documented, and reproducible analyses.
Ultimately, you will need to experiment with strategies that work for you -- what is most important is to develop a clear set of strategies and implement them tenaciously.
Below, we provide a few practical strategies to try as you begin developing your own workflows.

**Start with working code**
When building a workflow for the first time, creating an initial workflow based on a subset of your sample data can help verify that the workflow, tools, and command line syntax function at a basic level.
This functioning example code then provides a reliable workflow framework free of syntax errors which you can customize for your data without the overhead of generating correct workflow syntax from scratch.
**Table @tbl:workflows** provides links to official repositories containing tutorials and example biological analysis workflows, and workflow tutorials and code sharing websites like GitHub, GitLab, and Bitbucket have many publicly available workflows for other analyses.
If a workflow is available through Binder, you can test and experiment with workflow modification on Binder's cloud system without needing to install a workflow manager or software management tool [@doi:10.25080/Majora-4af1f417-011].


**Test with subsampled data**
While a workflow may run on test data, this is not a guarantee it will run on all data.
After verifying your chosen example workflow is functional, try running it with your own data or some public data related to your species or condition of interest.
Trying the workflow on a small subset of the data first can save time, energy, and computational resources.
For example, if working with FASTQ data, you can subsample the first million lines of a file (first 250k reads) by running:

`head -n 1000000 FASTQ_FILE.fq > test_fastq.fq`

While there are many more sophisticated ways to subsample reads, this technique should be sufficient for testing each step of a workflow prior to running your full dataset.

**Document your process**
Document your changes, explorations, and errors as you develop.
We recommend using the Markdown language so your documentation is in plain text to facilitate version control, but can still include helpful visual headings, code formatting, and embedded images.
Markdown editors with visual previewing, such as HackMD, can greatly facilitate notetaking, and Markdown documents are visually rendered properly within your online version control backups on services such as GitHub [@url:https://hackmd.io/].

**Develop your workflow**
From your working code, iteratively modify and add workflow steps to meet your data analysis needs.
This strategy allows you to find and fix mistakes on small sections of the workflow.
Periodically clean your output directory and rerun the entire workflow, to ensure all steps are fully interoperable (using small test data will improve the efficiency of this step!).
If possible, using mock or control datasets can help you verify that the analysis you are building actually returns correct biological results.
Tutorials and tool documentation are useful companions during development; as with any language, remembering workflow-specific syntax takes time and practice.

**Back up early and often**
As you write new code, back up your changes in an online repository such as GitHub, GitLab, or Bitbucket.
These services support both drag-and-drop and command line interaction.
Data backup will be discussed in the next section, [Data and resource management for workflow-enabled biology](## data-and-resource-management-for-workflow-enabled-biology).

**Scale up your workflow**
Bioinformatic tools vary in the resources they require: some analysis steps are compute-intensive, other steps are memory intensive, and still others will have large intermediate storage needs.
If using high-performance computing system or the cloud, you will need to request resources for running your pipeline, often provided as a simultaneous execution limit or purchased by your research group on a cost-per-compute basis.
Workflow systems provide built-in tools to monitor resource usage for each step.
Running a complete workflow on a single sample with resource monitoring enabled generates an estimate of computational resources needed for each step.
These estimates can be used to set appropriate resource limits for each step when executing the workflow on your remaining samples.
Strategies for resource management will be addressed in the next section, [Data and resource management for workflow-enabled biology](## data-and-resource-management-for-workflow-enabled-biology).

**Find a community and ask for help when you need it**
Local and online users groups are helpful communities when learning a workflow language.
When you are first learning, help from more advanced users can save you hours of frustration.
After you've progressed, providing that same help to new users can help you cement the syntax in your mind and tackle more advanced uses.
Data-centric workflow systems have been enthusiastically adopted by the open science community, and as a consequence, there is a critical mass of tutorials and open access code, as well as code discussion on forums and via social media, particularly Twitter.
Post in the relevant workflow forums when you have hit a stopping point you are unable to work through. Be respectful of people's time and energy and be sure to include appropriate details important to your problem (see [Strategic troubleshooting](## strategic-troubleshooting) section).
Troubleshooting strategies will be addressed in detail in the [Strategic troubleshooting](## strategic-troubleshooting) section.
